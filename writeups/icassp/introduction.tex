\section{Introduction}

The problem of identifying and extracting different speech signals 
recorded in a single, noisy environment has been widely
studied in the audio processing community \cite{loizou2007speech,hansler2008speech}. 
It becomes particularly challenging when only one microphone is used, or in the presence of 
non-stationary background noise, which is a very common situation in many applications encountered, e.g., in telephony.
We approach this problem as a monaural source separation method
by modeling the speech at an appropriate temporal resolution.
%This is a natural approach when the characteristics of both the signal of interest and the noise vary throughout time \cite{WilsonRSD08,JoderWEVS12,MysoreS11,DuanMS12}.
%Consequently, many works have used source separation techniques for addressing this problem,

The decomposition of time-frequency representations, such as the power or magnitude spectrogram
in terms of elementary atoms of a dictionary, has become a popular tool in audio processing. 
Non-negative matrix factorization (NMF) \cite{NMF, smaragdis2006probabilistic},
have been widely adopted in various audio processing tasks, including in particular source separation, see \cite{smaragdis2014static} for a recent review. 
There are many works that follow this line in speech separation \cite{schmidt06speechseparation,shashanka_icassp07} and enhancement \cite{JoderWEVS12,DuanMS12,schmidt07mlsp,mohammadiha2013supervised}. %and robust automatic speech recognition \cite{GemmekeVH11,WeningerWGSGHVR12},
%among many others.
%bandwidth extension \cite{BansalRS05,HanMP12} and speaker recognition \cite{wu2010robust,joder2012exploring}.

%NMF and PLCA produce high quality separation results when the dictionaries
%for different sources are sufficiently distinct.
%There is naturally a compromise between the approximation
%of the training data and tightness of the model: the more general is the dictionary the higher is the chance it will include elements that
%match spectral patterns in the competing sources.
%In order to mitigate this problem, recent approaches have proposed
%alternative models constraining the solution in meaningful ways,
%as for example, by imposing sparsity of the activations \cite{shashanka_icassp07,hoyer2004non}.
%

In plain NMF, signals that can be well approximated with the learned dictionary are
likely to resemble the training data on a frame by frame manner. They might, however, 
not be temporally consistent at larger temporal scales.
 Standard NMF approaches treat different time-frames independently, ignoring the 
temporal dynamics of the signals. In other words, there is additional structure in speech at a time-scale larger than
the frame-length that cannot be learned (or exploited) with NMF. In order to overcome this limitation,  
%and achieve consistency at a larger temporal scale,
many works have proposed regularized extensions of NMF to promote learned (or designed) structure in the codes. Examples of these approaches are, temporal smoothness of the activation coefficients \cite{fevotte2011majorization}, including  co-occurrence statistics of the basis functions \cite{WilsonRSD08}, and learned temporal dynamics \cite{MysoreS11,HanMP12,icassp13a,mohammadiha2013nonnegative}.
%Recent studies have proposed regularized variants of NMF or PLCA trying to overcome this limitation,

NMF-based source separation methods can be thought as the concatenation of two operators.
First, the signal is represented in a feature space given by a non-linear analysis operator, 
typically defined as the magnitude of a time-frequency representation such
as the Short-Time Fourier Transform (STFT). 
Then a synthesis operator, given by the dictionary learning stage, is applied to produce an unmixing in the feature space.
Finally, the separation is obtained by inverting these representations. 
Performing the separation in the non-linear representation is key to the success of the algorithm. The magnitude STFT is in general sparse (simplifying the separation process) and invariant to variations in the phase, thus relieving the NMF from
learning this irrelevant variability. 
This comes at the expense of inverting the unmixed estimates in the feature space, normally known as the
phase recovery problem \cite{yonina}. In the case of standard NMF, this is easily done via Wiener filtering technique discussed in Section \ref{sec:nmf}.

In this work, rather than seeking for a coding scheme with temporal regularity, we seek to encode a representation of the audio signal at a larger scale in which natrual variability in speech is highly compressible. 
Increasing the temporal scale of STFT or MEL representations either produces unstabilities due to 
variations in pitch and timbre, or looses important discriminative information \cite{deepscatt}. 
In order to overcome the limitations of these shallow representations, 
scattering transforms \cite{pami, deepscatt} cascade several 
stages of complex wavelet decompositions and complex modulus, achieving 
discriminative representations with the ability to capture temporal structures at larger scales, 
e.g. smooth changes in pitch and envelope. 
Scattering transforms achieve state-of-the-art results on auditory texture discrimination, and music genre recognition \cite{deepscatt, phdjoan}.
A dictionary learnt to represent the signal in this deep representation would implicitly be learning the short term temporal dynamics of the signal.

%Scattering transforms have recently been introduced \cite{XX} to represent audio signals and images, A scattering transform iterates on complex wavelet transforms and modulus operators which compute their envelop. It has close relations with psychophysical and physiological models \cite{XX}. %Their success in audio classification comes from the fact that they can reduce the intra class variability (like temporal redundancy).

Our claim is that an important part of the consistency that is imposed via structured NMF, can be eliminated with a better signal representation.
In this new setting one can learn the temporal dynamics with a very simple NMF encoding. However, the problem that becomes
more difficult is that of inverting the representation. Recent studies in textured sound synthesis from scattering
moments have solved this problem successfully using gradient descent algorithms \cite{bruna2013audio}.

Synthesis models with coherent dictionaries are known to be highly unstable representations \cite{jenatton2012local}. 
Thus, training them to satisfy slowness and temporal consistency can be challenging. For example, in \cite{icassp13a}
the authors explain that the learning the temporal dynamics in NMF via a Kalman type of model, can become very difficult
when the coding is sparse due to the instability and jitter in the codes.
In contrast, analysis operators are stable by construction.

Section \ref{nmfsec} reviews non-negative matrix factorization, while section \ref{scattsec} describes scattering 
representations for speech. Our source separation algorithm is described in Section \ref{algosec} and 
numerical experiments on TIMIT and Grid datasets are reported in Section \ref{resultssec}.

%In all these methods the audio signal is represented as a non-negative and the demixng is obtained  model is expressed as the minimization of a cost with a data fitting term and some structure-promoting penalties.



